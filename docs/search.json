[
  {
    "objectID": "lab-6b.html",
    "href": "lab-6b.html",
    "title": "Lab 6: Machine Learning â€“ ESS 330",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.4     âœ” tidyr     1.3.1\nâœ” purrr     1.0.4     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(readr)\nlibrary(tidymodels)\n\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.3.0 â”€â”€\nâœ” broom        1.0.7     âœ” rsample      1.2.1\nâœ” dials        1.4.0     âœ” tune         1.3.0\nâœ” infer        1.0.7     âœ” workflows    1.2.0\nâœ” modeldata    1.4.0     âœ” workflowsets 1.1.0\nâœ” parsnip      1.3.1     âœ” yardstick    1.3.2\nâœ” recipes      1.2.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\nâœ– scales::discard() masks purrr::discard()\nâœ– dplyr::filter()   masks stats::filter()\nâœ– recipes::fixed()  masks stringr::fixed()\nâœ– dplyr::lag()      masks stats::lag()\nâœ– yardstick::spec() masks readr::spec()\nâœ– recipes::step()   masks stats::step()\n\n\nCode\nlibrary(baguette)\n\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n\nCode\nlibrary(glue)\n\n\nWarning: package 'glue' was built under R version 4.4.3\n\n\nCode\nlibrary(powerjoin)\n\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\n\nCode\nlibrary(ggthemes)\n\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\n\nCode\nlibrary(patchwork)\n\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\n\nCode\nlibrary(scales)"
  },
  {
    "objectID": "lab-6b.html#q1-download-data",
    "href": "lab-6b.html#q1-download-data",
    "title": "Lab 6: Machine Learning â€“ ESS 330",
    "section": "ðŸ”¹ Q1: Download Data",
    "text": "ðŸ”¹ Q1: Download Data\n\n\nCode\n# Create data folder\nif (!dir.exists(\"data\")) dir.create(\"data\")\n\n# Download the PDF documentation\npdf_url &lt;- \"https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf\"\ndownload.file(pdf_url, destfile = \"data/camels_attributes_v2.0.pdf\")\n\n# Download all data types\nroot &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files &lt;- paste0(root, \"/camels_\", types, \".txt\")\nlocal_files  &lt;- paste0(\"data/camels_\", types, \".txt\")\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge the data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\ncamels &lt;- power_full_join(camels, by = 'gauge_id')\n\n# Add log of q_mean\ncamels &lt;- camels |&gt; mutate(logQmean = log(q_mean))"
  },
  {
    "objectID": "lab-6b.html#q2-make-2-maps",
    "href": "lab-6b.html#q2-make-2-maps",
    "title": "Lab 6: Machine Learning â€“ ESS 330",
    "section": "ðŸ”¹ Q2: Make 2 Maps",
    "text": "ðŸ”¹ Q2: Make 2 Maps\n\n\nCode\np1 &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_viridis_c() +\n  ggtitle(\"Aridity across CAMELS sites\") +\n  theme_map()\n\np2 &lt;- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_viridis_c() +\n  ggtitle(\"Mean Precipitation (p_mean)\") +\n  theme_map()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nExplanation: These maps help us understand geographic patterns in aridity and precipitation, which are key predictors of streamflow."
  },
  {
    "objectID": "lab-6b.html#q3-build-xgboost-and-neural-net-models",
    "href": "lab-6b.html#q3-build-xgboost-and-neural-net-models",
    "title": "Lab 6: Machine Learning â€“ ESS 330",
    "section": "ðŸ”¹ Q3: Build xgboost and neural net models",
    "text": "ðŸ”¹ Q3: Build xgboost and neural net models\n\n\nCode\n# Split data for modeling\nset.seed(42)\nsplit &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(split)\ncamels_test &lt;- testing(split)\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# Define recipe\nrec &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train) |&gt; \n  step_log(all_predictors()) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\n# Define models\nrf_model &lt;- rand_forest(trees = 500) |&gt; set_engine(\"ranger\", importance = \"impurity\") |&gt; set_mode(\"regression\")\nxgb_model &lt;- boost_tree(trees = 1000, learn_rate = 0.05) |&gt; set_engine(\"xgboost\") |&gt; set_mode(\"regression\")\nnnet_model &lt;- bag_mlp() |&gt; set_engine(\"nnet\") |&gt; set_mode(\"regression\")\n\n# Build workflow set\nwf_set &lt;- workflow_set(\n  preproc = list(rec),\n  models = list(rf_model, xgb_model, nnet_model)\n) |&gt; \n  workflow_map(\"fit_resamples\", resamples = camels_cv)\n\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\nWarning: package 'nnet' was built under R version 4.4.3\n\n\nCode\n# Visualize results\nautoplot(wf_set)\n\n\n\n\n\n\n\n\n\nCode\nrank_results(wf_set, rank_metric = \"rsq\", select_best = TRUE)\n\n\n# A tibble: 6 Ã— 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Preproâ€¦ rmse    0.511  0.0327    10 recipe       bag_â€¦     1\n2 recipe_bag_mlp    Preproâ€¦ rsq     0.783  0.0360    10 recipe       bag_â€¦     1\n3 recipe_rand_foreâ€¦ Preproâ€¦ rmse    0.541  0.0342    10 recipe       randâ€¦     2\n4 recipe_rand_foreâ€¦ Preproâ€¦ rsq     0.767  0.0387    10 recipe       randâ€¦     2\n5 recipe_boost_tree Preproâ€¦ rmse    0.622  0.0370    10 recipe       boosâ€¦     3\n6 recipe_boost_tree Preproâ€¦ rsq     0.711  0.0430    10 recipe       boosâ€¦     3\n\n\nExplanation: The autoplot shows cross-validated RMSE and RÂ² for each model. I select the best model based on RÂ²."
  },
  {
    "objectID": "lab-6b.html#q4-build-your-own",
    "href": "lab-6b.html#q4-build-your-own",
    "title": "Lab 6: Machine Learning â€“ ESS 330",
    "section": "ðŸ”¹ Q4: Build your own",
    "text": "ðŸ”¹ Q4: Build your own\n\nQ4a: Data Splitting\n\n\nCode\nset.seed(101)\nsplit &lt;- initial_split(camels, prop = 0.75)\ntrain &lt;- training(split)\ntest &lt;- testing(split)\ncv &lt;- vfold_cv(train, v = 10)\n\n\n\n\nQ4b: Recipe\n\n\nCode\n# Ensure 'train' is already defined before this step\nmy_recipe &lt;- recipe(logQmean ~ aridity + p_mean + elev_mean + slope_mean, data = train) |&gt; \n  step_log(all_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\n\n\n\nQ4c: Define 3 Models\n\n\nCode\nmodel_rf &lt;- rand_forest(trees = 500) |&gt; set_engine(\"ranger\") |&gt; set_mode(\"regression\")\nmodel_xgb &lt;- boost_tree(trees = 1000, learn_rate = 0.05) |&gt; set_engine(\"xgboost\") |&gt; set_mode(\"regression\")\nmodel_nn &lt;- bag_mlp() |&gt; set_engine(\"nnet\") |&gt; set_mode(\"regression\")\n\n\n\n\nQ4d: Workflow Set\n\n\nCode\nmy_wf_set &lt;- workflow_set(\n  preproc = list(my_recipe),\n  models = list(model_rf, model_xgb, model_nn)\n) |&gt; \n  workflow_map(\"fit_resamples\", resamples = cv)\n\n\n\n\nQ4e: Evaluation\n\n\nCode\nautoplot(my_wf_set)\n\n\n\n\n\n\n\n\n\nCode\nrank_results(my_wf_set, rank_metric = \"rsq\", select_best = TRUE)\n\n\n# A tibble: 6 Ã— 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Preproâ€¦ rmse    0.330  0.0247    10 recipe       bag_â€¦     1\n2 recipe_bag_mlp    Preproâ€¦ rsq     0.923  0.0109    10 recipe       bag_â€¦     1\n3 recipe_rand_foreâ€¦ Preproâ€¦ rmse    0.349  0.0307    10 recipe       randâ€¦     2\n4 recipe_rand_foreâ€¦ Preproâ€¦ rsq     0.910  0.0135    10 recipe       randâ€¦     2\n5 recipe_boost_tree Preproâ€¦ rmse    0.387  0.0278    10 recipe       boosâ€¦     3\n6 recipe_boost_tree Preproâ€¦ rsq     0.895  0.0127    10 recipe       boosâ€¦     3\n\n\nExplanation: This step shows model performance across resamples. I choose the model with the highest RÂ².\n\n\nQ4f: Extract and Evaluate Best Model on Test Set\n\n\nCode\nbest_model_id &lt;- pull(rank_results(my_wf_set, rank_metric = \"rsq\", select_best = TRUE), wflow_id)[1]\nfinal_wf &lt;- extract_workflow(my_wf_set, id = best_model_id)\nfinal_fit &lt;- final_wf |&gt; fit(data = train)\nfinal_data &lt;- augment(final_fit, new_data = test)\n\n# Evaluate\nmetrics(final_data, truth = logQmean, estimate = .pred)\n\n\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.336\n2 rsq     standard       0.920\n3 mae     standard       0.214\n\n\nCode\n# Plot\nggplot(final_data, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point() +\n  geom_abline(linetype = 2, color = \"red\") +\n  theme_linedraw() +\n  labs(title = \"Observed vs Predicted Log Mean Flow\",\n       x = \"Observed\",\n       y = \"Predicted\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nExplanation: This final step shows how well the best model generalizes to unseen data. A tight cluster along the 1:1 line indicates strong predictive performance."
  }
]