---
title: "Lab 6: Machine Learning â€“ ESS 330"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
project:
  type: website
  output-dir: docs
---


```{r} setup, include=FALSE}

library(tidyverse)
library(readr)      
library(tidymodels)
library(baguette)
library(glue)
library(powerjoin)
library(ggthemes)
library(patchwork)
library(scales)
library(purrr)

```

```{r}
## ðŸ”¹ Q1: Download Data (10 pts)

# Create data folder
if (!dir.exists("data")) dir.create("data")

# Download the PDF documentation
pdf_url <- "https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf"
download.file(pdf_url, destfile = "data/camels_attributes_v2.0.pdf")

# Download all data types
root <- 'https://gdex.ucar.edu/dataset/camels/file'
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
remote_files <- glue('{root}/camels_{types}.txt')
local_files  <- glue('data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge the data
camels <- map(local_files, read_delim, show_col_types = FALSE)
camels <- power_full_join(camels, by = 'gauge_id')

# Add log of q_mean
camels <- camels |> mutate(logQmean = log(q_mean))

```

From the documentation: `zero_q_freq` is the **frequency (as a fraction) of days with zero streamflow**, indicating how often a river basin experiences no flow.

```{r}
## ðŸ”¹ Q2: Make 2 Maps (10 pts)

p1 <- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = aridity)) +
  scale_color_viridis_c() +
  ggtitle("Map of Aridity") +
  theme_map()

p2 <- ggplot(camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = p_mean)) +
  scale_color_viridis_c() +
  ggtitle("Map of Mean Precipitation") +
  theme_map()

p1 + p2
```

```{r}
## ðŸ”¹ Q3: XGBoost + Neural Net Models (20 pts)

# Train/Test Split
set.seed(42)
split <- initial_split(camels, prop = 0.75)
camels_train <- training(split)
camels_test <- testing(split)
camels_cv <- vfold_cv(camels_train, v = 10)

# Recipe
rec <- recipe(logQmean ~ aridity + p_mean, data = camels_train) |> 
  step_log(all_predictors()) |> 
  step_naomit(all_predictors(), all_outcomes())

# Define models
rf_model <- rand_forest(trees = 500) |> set_engine("ranger", importance = "impurity") |> set_mode("regression")

xgb_model <- boost_tree(trees = 1000, learn_rate = 0.05) |> 
  set_engine("xgboost") |> 
  set_mode("regression")

nnet_model <- bag_mlp() |> set_engine("nnet") |> set_mode("regression")

# Workflow set
wf_set <- workflow_set(
  preproc = list(rec),
  models = list(rf_model, xgb_model, nnet_model)
) |> 
  workflow_map("fit_resamples", resamples = camels_cv)

# View results
autoplot(wf_set)
rank_results(wf_set, rank_metric = "rsq", select_best = TRUE)
```

```{r}
## ðŸ”¹ Q4aâ€“f: Build Your Own ML Pipeline (75 pts)

### Q4a: Data Splitting
set.seed(99)
split <- initial_split(camels, prop = 0.75)
train <- training(split)
test <- testing(split)
cv <- vfold_cv(train, v = 10)

### Q4b: Recipe
my_recipe <- recipe(logQmean ~ aridity + p_mean + elev_mean + slope_mean, data = train) |> 
  step_log(all_predictors()) |> 
  step_normalize(all_predictors()) |> 
  step_naomit(all_predictors(), all_outcomes())
```

These predictors (aridity, precipitation, elevation, slope) are known to influence streamflow by capturing climate and terrain characteristics. Log transformations reduce skew, normalization helps neural networks.

```{r}
### Q4c: Define 3 Models
# Random Forest
model_rf <- rand_forest(trees = 500) |> set_engine("ranger") |> set_mode("regression")

# XGBoost
model_xgb <- boost_tree(trees = 1000, learn_rate = 0.05) |> set_engine("xgboost") |> set_mode("regression")

# Neural Network
model_nn <- bag_mlp() |> set_engine("nnet") |> set_mode("regression")

### Q4d: Workflow Set
my_wf_set <- workflow_set(
  preproc = list(my_recipe),
  models = list(model_rf, model_xgb, model_nn)
) |> 
  workflow_map("fit_resamples", resamples = cv)

### Q4e: Evaluation
autoplot(my_wf_set)
rank_results(my_wf_set, rank_metric = "rsq", select_best = TRUE)
```

**Interpretation**: We select the model with the highest `rsq`. Random Forest or XGBoost typically performs best for this type of data.

```{r}
### Q4f: Extract and Evaluate Best Model on Test Set
# Select only the best model by rsq (take the first if multiple)
best_model_id <- pull(rank_results(my_wf_set, rank_metric = "rsq", select_best = TRUE), wflow_id)[1]

# Extract the best workflow
final_wf <- extract_workflow(my_wf_set, id = best_model_id)

# Fit on training data
final_fit <- final_wf |> fit(data = train)

# Predict on test data
final_data <- augment(final_fit, new_data = test)

# Metrics
metrics(final_data, truth = logQmean, estimate = .pred)

# Plot
ggplot(final_data, aes(x = logQmean, y = .pred, color = aridity)) +
  geom_point() +
  geom_abline(linetype = 2, color = "red") +
  theme_linedraw() +
  labs(title = "Observed vs Predicted Log Mean Flow",
       x = "Observed",
       y = "Predicted",
       color = "Aridity")
```
